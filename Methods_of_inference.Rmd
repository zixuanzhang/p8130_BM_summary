---
title: "Methods of inference"
date: 2018-10-04
output: 
     html_document:
       toc: true
       toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Sampling Distribution

Rationale:

We are insterested in the population parameters: $\mu, \sigma^2, p$. We need to get observed values (statistics) from samples in order to make inference about the population parameters. The distribution of statistics obtained fromn samples are called __sampling distribution__.

### Sampling distribution

1. estimation of mean $\mu$ is $\overline{X}$

$$\overline{X} = \frac{\sum\limits_{i=1}^n{X_i}}{n}$$

2. why $\overline{X}$ is a good estimator of $\mu$

*    $E(\overline{X}) = \mu$ means this is an unbiased estimator
*    the standard deviation of estimator $\overline{X}$ is small

3. distribution of $\overline{X}$

if n is large enough (n >= 30), then the underlying distribution of $\overline{X}$ is normal by CLT. 
$$\overline{X} \sim N(\mu,\frac{\sigma^2}{n} ) $$

# statistical inference

*    point estimation
*    interval estimation
*    hypothesis testing
*    prediction

# point estimation

for example, $\overline{X}$ can be a point estimate for $\mu$. But point estimate is generally speaking not the exact value of true parameter, so we need to construct condidence interval for the point estimate which contains the plausible values for population parameter.

# interval estimation

### One sample mean, known variance

At significance level $\alpha$ 

$$CI: \overline{X} \pm z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}}$$

$$ \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim N(0,1)$$

Example: $\overline{X} = 175$, $\sigma = 15$, $n = 10$, $\overline{X} \sim N(175, \frac{15^2}{10})$
then the 95% CI for the true mean is calculated as follow:

```{r}
lb <- 175 - qnorm(0.975) * 15/sqrt(10)
up <- 175 + qnorm(0.975) * 15/sqrt(10)
c(lb, up)
```

What if we want a 99% CI?

```{r}
lb <- 175 - qnorm(0.995) * 15/sqrt(10)
up <- 175 + qnorm(0.995) * 15/sqrt(10)
c(lb, up)
```

we can see that as confidence level increase, the CI become narrower.

### One sample mean, unknown variance

Since population variance is unknown, we need to use $s^2$ as an estimate for $\sigma^2$

$$ \frac{\overline{X} - \mu}{s/\sqrt{n}} \sim t_{n-1}$$

$$CI: \overline{X} \pm t_{n-1, 1-\alpha/2} \frac{s}{\sqrt{n}}$$

Example: $\overline{X} = 175$, $s = 15$,$n = 10$, $\overline{X} \sim N(175, \frac{15^2}{10})$

Calculate the 95% confidence interval:

```{r}
lb <- 175 - qt(0.975, df=9) * 15/sqrt(10)
up <- 175 + qt(0.975, df=9) * 15/sqrt(10)
c(lb, up)
```

### variance estimation

The sample variance $s^2$ is an unbiased estimator for population variance $\sigma^2$

CI for variance:

### confidence interval illustration



### look at Z distribution and t distribution

---------------------------------------------------------------------------------------------------------
# Hypothesis testing

### One sample, one sided tests, with known variance, normal distribution

Test for __mean__ of normal distribution with __known variance__

State hypothesis:
$$ H_0: \mu = \mu_0$$
$$H_1: \mu < \mu_0$$

Compute test statistics:
$$z = \frac{\overline{X} - \mu_0}{\sigma/\sqrt{n}} \sim N(0,1) $$

Reject H0 if $$z < -z_{\alpha}$$

### One sample, 2-sided test, with known variance, normal distribution

$$ H_0: \mu = \mu_0$$
$$H_1: \mu \ne \mu_0$$

test statistics:

$$z = \frac{\overline{X} - \mu_0}{\sigma/\sqrt{n}} \sim N(0,1) $$

reject H0 if $$|z| > z_{1-\alpha/2}$$

### One sample, 1 sided, unknown variance

Hypothesis:
$$ H_0: \mu = \mu_0$$

$$H_1: \mu < \mu_0$$

test statistics:
$$t = \frac{\overline{X} - \mu_0}{s/\sqrt{n}} \sim t_{n-1} $$

Reject H0 if: $$t < -t_{n-1, \alpha}$$

### One sample, 2 sided, unknown variance

$$ H_0: \mu = \mu_0$$

$$H_1: \mu \ne \mu_0$$



test statistics: $$t = \frac{\overline{X} - \mu_0}{s/\sqrt{n}} \sim t_{n-1} $$

reject H0 if: $$|t| > t_{n-1, 1-\alpha/2}$$

### hypothesis testing code in R

Example 1: infarct size in patients; 
we want to know if the infact size in the treated patients truely different from the mean of untreated patients, so this is a __2 sided test__

Given that $\overline{X} = 16$, $ n = 40$, $\mu = 16$, $\alpha = 0.05$

Hypothsis: $$H_0: \mu = 25$$

$$H_a : \mu \ne 25$$

Since we do not know population variance, we will use t test statistics:
$$t = \frac{16-25}{10/\sqrt{40}} = -5.6921$$

then compare with critical value of $t_{0.975,39}$

```{r}
qt(0.975, 39)
```

Thus $|t| > 2.02 $, we should reject null.

we can also compute p-value:

```{r}
2*pt(-5.6921,39)
```

then we get very small p value, which leads to rejection of H0

----------------
Example 2: # We want to test the hypothesis that mothers with low socio-economic status deliver babies with lower than 'normal' birthweights.  Suppose the true mean birthweight follows a normal distribution with mean of 120oz and standard deviation of 24oz.

Hypothesis: $$\mu = 120$$

$$\mu < 120$$

Since we don't know the population variance, then we use $s^2 = 24^2$, we will use t test.

First, we generate a sample of 50 from normal distribution $N(120, 24)$

```{r}
set.seed(3)
bweight <- rnorm(50,120,24)
```

then we can do t test on this "observed" data

```{r}
t.test(bweight, alternative='two.sided', mu = 120)
```

since the p value is large, then we do not reject the null.

If we want to do one sided test: either greater or less than the population mean

```{r}
t.test(bweight, alternative = 'less', mu = 120)
t.test(bweight, alternative = 'greater', mu = 120)
```


# Two sample means hypothesis testing
----------------------
### paired test

Example: test for cholesterol level reduction

```{r data entry}
LDL <- tibble(weight_before = c(201,231,221,260,228,237,326,235,240,267,284,201),
              weight_after = c(200,236,216,233,224,216,296,195,207,247,210,209),
              d = weight_after - weight_before)
LDL
```

Hypothesis testing: 
$$H_0 : \mu_1 = \mu_2$$
$$H_a: \mu_1 \ne \mu_2$$

Since we do not know population variance, we will use one sample t test for __d__.
Testing statistics: $$t = \frac{\overline{d}-0}{s_d/\sqrt{n}}$$

Do the calculation manually:
```{r}
s_d <- sd(LDL$d)
se <- s_d/sqrt(length(LDL$d))
t <- mean(LDL$d)/se
t

# find critical value
qt(0.975, length(LDL$d)-1)

# confidence interval
lb <- mean(LDL$d) - qt(0.975, length(LDL$d)-1) * se
up <- mean(LDL$d) + qt(0.975, length(LDL$d)-1) * se
c(lb,up)
```

Since $|t| > t_{0.975, 11}$, we have evidence to reject the null. 


Or we can do the paired t test in R, and arrive at the same conclusion

```{r}
t.test(LDL$weight_after, LDL$weight_before, paired=T) # difference = after - before
```

### two independent sample mean test

Example: oral contraceptive

|        | n   |  $\overline{X}$ | s  |
|-------|------|-----------------|-----|
|Non OC | 10   |    1.08         | 0.16 |
|  OC   | 10   |     1           | 0.14 |

Goal: Is there any difference in this two groups?

$$H_0 : \mu_1 = \mu_2$$
$$H_a: \mu_1 \ne \mu_2$$

Assume that the BMD in two populations are normally distributed, then $X_1 \sim N(\mu_1, \sigma_1^2)$, $X_2 \sim N(\mu_2, \sigma_2^2)$.

First we need test if these population have equal variance:  
$H_0 : \sigma_1^2 = \sigma_2^2$ vs. $H_a : \sigma_1^2 \ne \sigma_2^2$

Calculate F test statistics: $$F = \frac{s_1^2}{s_2^2} \sim F_(n_1-1, n_2-1)$$


```{r}
F_test <- 0.16^2 / 0.14^2
F_test

F_crit <- qf(0.975, df1 = 9, df2 = 9)
F_crit
```

Since $F_test < F_{critical} $, we fail to reject the null. We do not have sufficient evidence to declare the difference between two population variances.

So now we can do __two sided__ t test for two independent populations with equal population variance (unknown)

Calculate test statistics: $$t = \frac{\overline{X_1}-\overline{X_2}}{s\sqrt{1/n_1+1/n_2}}$$ 

where the pooled variance is calculated as follow:
$$s_{pooled}^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2} $$

```{r}
s_pool = sqrt((9*0.16^2 + 9*0.14^2)/18)
s_pool

se <- s_pool * sqrt(1/10 + 1/10)
t_stat <- (1.08 - 1)/se

t_stat

t_crit <- qt(0.975, 18)
t_crit
```

Since $t_{stat} < t_{crit} $, we cannot reject the null. 
So at the significance level of 0.05, there is not enough evidence for us to conclude the mean BMD of Non OC group is different from that of OC group.

Find 95% Confidence Interval :

```{r}
lb <- 1.08 - 1 - t_crit * se
up <- 1.08 - 1 + t_crit * se
c(lb, up)
```


---------------------
Example 2: # Effect of caffeine on muscle metabolism.

15 men were randomly selected to take a capsule containing pure caffeine one hour before the test.
The other group of 20 men received a placebo capsule. 
During each exercise the subject's respiratory exchange ratio (RER) was measured.
The question of interest to the experimenter was whether, on average, caffeine consumption has an effect on RER.
The two samples came from two underlying normal distributions: N(94.2,5.6), N(105.5,8.1) and are independent.

```{r}
# generate two sample
set.seed(2)
caff <- rnorm(15, 94.2, 5.6)
placebo <- rnorm(20, 105.5, 8.1)
```

First, we need to test for equality of two variances

```{r}
var.test(placebo,caff, alternative = "two.sided")
```

we saw that $p_{value} = 0.03128 < 0.05$, so we will reject the null conclude that at significance level of 0.05, these two populations variances are not equal. 

then we can set up the testing:

```{r}
t_result <- t.test(caff, placebo, var.equal = FALSE, paired = FALSE) 
t_result
```

based on the result, we saw that $p < 0.025$, so we should reject the null and conclude that at significance level of 0.05, the mean of two groups are different. 
The confidence interval is (-16.89, -6.125), so we could safely say that the mean of RER in caffeine group is lower. 




